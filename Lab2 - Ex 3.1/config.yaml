#### Weight & Biases settings

wandb:
  name: "2024-06-03 - Second run"
  resume: True
  job_type: "Fine-Tuning"
  group: "Exercise 3.1"
  project: "University - DLA - Lab 2"
  tags: 
  - "DistilRoBERTa"
  - "BERT"
  - "NLP"
  - "Base"
  - "PEFT"
  - "LoRA"
  notes: "Fine-tuning DistilRoBERTa on an NLP dataset."


#### Experiment settings

config:
  # Machine
  seed: 1
  device: "cuda:0"

  # Paths and dirs
  data_dir: "data"
  results_dir: "results"
  log_file: "log.txt"
  hf_cache_dir: "./hf"

  model_type: "BERT"
  model:
    model_name: "distilroberta-base"
    output_size: 10
    init_mode: "xavier"
    init_dist: "normal"
    device_map: 'auto'
    freeze_model_base: False
    peft: "LoRA"

  criterion: "CrossEntropyLoss"

  dataset:
    dataset_type: "TextClassification"
    dataset_name: "yelp_review_full"
    filename: null
    train_set_name: "train"
    test_set_name: "test"
    val_set_name: null
    val_size: 0.2
    padding_side: 'right'
    trunc_side: 'right'
    max_token_len: 128
    val_shuffle: True
    use_sampler: True
  
  dataloader:
    num_workers: 8
    batch_size: 256
    pin_memory: True
    persistent_workers: True

  optimizer_name: "AdamW"  # Lion, Adam, SGD, Adamax, Adadelta, RMSProp ecc. (https://pytorch.org/docs/stable/optim.html)
  optimizer:
    lr: 0.00005
    weight_decay: 0.01
    amsgrad: True
  
  scheduler_name: "OneCycleLR"  # MultiStepLR
  scheduler: 
    max_lr: 0.0001
  
  pretrain: False
  pretest: False
  train: True
  test: True

  training:
    n_epochs: 500
    eval_interval: 5
    checkpoint_every: 10
    patience: 30

  clip_grads: False

  # Goals
  problem: None
  xai: False
  explain_gradients: False
  explain_cam: False