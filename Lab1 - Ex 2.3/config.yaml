#### Weight & Biases settings

wandb:
  name: "GradCAM - CNN - 9 Conv Layers - Test"
  resume: False
  job_type: "XAI"
  group: "Exercise 2.3"
  project: "University - DLA - Lab 1"
  tags: 
  - "CNN"
  - "ResNet"
  - "Base"
  notes: "Explaining Predictions with Class Activation Mappings."


#### Experiment settings

config:
  # Machine
  seed: 1
  device: "cuda:1"
  cuda_benchmark: True

  # Paths and dirs
  data_dir: "../data"
  results_dir: "results"
  log_file: "log.txt"
  # hf_cache_dir: "../hf"

  model_type: "CNN"
  model:
    in_channels: 3
    depth: 9
    output_size: 10
    want_shortcut: False
    activation: "ReLU"
    fc_activation: "ReLU"
    pool_type: "maxpool"
  
  criterion: "CrossEntropyLoss"

  dataset:
    dataset_type: "Vision"
    dataset_name: "CIFAR10"
    val_size: 0.2
    val_shuffle: True
    normalize: True
    augment: True
    use_sampler: True
  
  dataloader:
    num_workers: 8
    batch_size: 256
    pin_memory: True
    persistent_workers: True

  optimizer_name: "AdamW"  # Lion, Adam, SGD, Adamax, Adadelta, RMSProp ecc. (https://pytorch.org/docs/stable/optim.html)
  optimizer:
    lr: 0.00005
    weight_decay: 0.01
    amsgrad: True
  
  scheduler_name: "OneCycleLR"  # MultiStepLR
  scheduler: 
    max_lr: 0.0001
  
  pretrain: False
  pretest: False
  train: False
  test: False
  test_original: False

  training:
    n_epochs: 150
    eval_interval: 1
    checkpoint_every: 1
    clip_grads: False
    patience: 20


  # Goals
  problem: null
  xai: True
  explain_gradients: False
  explain_cam: True