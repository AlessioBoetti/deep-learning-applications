#### Weight & Biases settings

name: "2024-03-11 - First run"
job_type: "Train-Test"
group: "Exercise 1"
project: "University - DLA - Lab 2"
tags: 
- "LLM"
- "GPT"
- "Text"
- "Base"
- "May"
notes: "First attempt at training NanoGPT from Andrew Karpathy YouTube tutorial"

resume: False

config:
  architecture: "NanoGPT"
  # depth: 9
  # n_classes: 10
  # want_shortcut: False
  # activation: "ReLU"
  # fc_activation: "ReLU"
  # pooling: "maxpool"
  # clip_grads: True

  # datasets
  # dataset_name: "CIFAR10"
  # val_size: 0.2
  # val_shuffle: True
  # normalize: True
  # augment: True
  # use_sampler: True

  # hyperparams
  # optimizer_name: "amsgrad"  # Lion, Adam
  # criterion: "Cross-Entropy Loss"
  # lr: 0.00005
  # weight_decay: 0.01
  # batch_size: 512
  # scheduler: "One Cycle Learning"
  # max_lr: 0.0001
  # n_epochs: 150
  # patience: 30

  # run type
  # evaluation: False
  # pretrain: False
  # train: True
  # test: True



#### Standard Settings

seed: 1
device: "cpu"
n_workers_dataloader: 0

# Paths and dirs
data: "divina_commedia.txt"
results_dir: "results"
log_file: "log.txt"

# Setting
problem: None
xai: False
explain_gradients: False
explain_cam: False