2024-05-16 23:37:49,653 - Starting run.
2024-05-16 23:37:49,653 - Logger setup correctly.
2024-05-16 23:37:49,661 - Seed set to 1.
2024-05-16 23:37:49,661 - Log filepath: results\2024-05-17 - First run\log.txt.
2024-05-16 23:37:49,661 - Network: NanoGPT
2024-05-16 23:37:49,661 - Computation device: cuda:0
2024-05-16 23:37:49,661 - Loading divina_commedia.txt
2024-05-16 23:37:49,663 - Data loaded.
2024-05-16 23:37:49,707 - Initializing NanoGPT model.
2024-05-16 23:37:50,588 - Model initialized.
2024-05-16 23:37:50,588 - Showing model structure:
2024-05-16 23:37:50,589 - GPTLanguageModel(
  (token_embedding_table): Embedding(58, 384)
  (position_embedding_table): Embedding(256, 384)
  (blocks): Sequential(
    (0): TransformerBlock(
      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (sa): MultiHeadAttention(
        (heads): ModuleList(
          (0-5): 6 x Head(
            (key): Linear(in_features=384, out_features=64, bias=False)
            (query): Linear(in_features=384, out_features=64, bias=False)
            (value): Linear(in_features=384, out_features=64, bias=False)
            (dropout): Dropout(p=0.2, inplace=False)
          )
        )
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
      )
      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (ffwd): FeedFoward(
        (fc): Sequential(
          (0): Linear(in_features=384, out_features=1536, bias=True)
          (1): ReLU()
          (2): Linear(in_features=1536, out_features=384, bias=True)
          (3): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (1): TransformerBlock(
      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (sa): MultiHeadAttention(
        (heads): ModuleList(
          (0-5): 6 x Head(
            (key): Linear(in_features=384, out_features=64, bias=False)
            (query): Linear(in_features=384, out_features=64, bias=False)
            (value): Linear(in_features=384, out_features=64, bias=False)
            (dropout): Dropout(p=0.2, inplace=False)
          )
        )
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
      )
      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (ffwd): FeedFoward(
        (fc): Sequential(
          (0): Linear(in_features=384, out_features=1536, bias=True)
          (1): ReLU()
          (2): Linear(in_features=1536, out_features=384, bias=True)
          (3): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (2): TransformerBlock(
      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (sa): MultiHeadAttention(
        (heads): ModuleList(
          (0-5): 6 x Head(
            (key): Linear(in_features=384, out_features=64, bias=False)
            (query): Linear(in_features=384, out_features=64, bias=False)
            (value): Linear(in_features=384, out_features=64, bias=False)
            (dropout): Dropout(p=0.2, inplace=False)
          )
        )
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
      )
      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (ffwd): FeedFoward(
        (fc): Sequential(
          (0): Linear(in_features=384, out_features=1536, bias=True)
          (1): ReLU()
          (2): Linear(in_features=1536, out_features=384, bias=True)
          (3): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (3): TransformerBlock(
      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (sa): MultiHeadAttention(
        (heads): ModuleList(
          (0-5): 6 x Head(
            (key): Linear(in_features=384, out_features=64, bias=False)
            (query): Linear(in_features=384, out_features=64, bias=False)
            (value): Linear(in_features=384, out_features=64, bias=False)
            (dropout): Dropout(p=0.2, inplace=False)
          )
        )
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
      )
      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (ffwd): FeedFoward(
        (fc): Sequential(
          (0): Linear(in_features=384, out_features=1536, bias=True)
          (1): ReLU()
          (2): Linear(in_features=1536, out_features=384, bias=True)
          (3): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (4): TransformerBlock(
      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (sa): MultiHeadAttention(
        (heads): ModuleList(
          (0-5): 6 x Head(
            (key): Linear(in_features=384, out_features=64, bias=False)
            (query): Linear(in_features=384, out_features=64, bias=False)
            (value): Linear(in_features=384, out_features=64, bias=False)
            (dropout): Dropout(p=0.2, inplace=False)
          )
        )
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
      )
      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (ffwd): FeedFoward(
        (fc): Sequential(
          (0): Linear(in_features=384, out_features=1536, bias=True)
          (1): ReLU()
          (2): Linear(in_features=1536, out_features=384, bias=True)
          (3): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (5): TransformerBlock(
      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (sa): MultiHeadAttention(
        (heads): ModuleList(
          (0-5): 6 x Head(
            (key): Linear(in_features=384, out_features=64, bias=False)
            (query): Linear(in_features=384, out_features=64, bias=False)
            (value): Linear(in_features=384, out_features=64, bias=False)
            (dropout): Dropout(p=0.2, inplace=False)
          )
        )
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
      )
      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (ffwd): FeedFoward(
        (fc): Sequential(
          (0): Linear(in_features=384, out_features=1536, bias=True)
          (1): ReLU()
          (2): Linear(in_features=1536, out_features=384, bias=True)
          (3): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (layer_norm_final): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (lm_head): Linear(in_features=384, out_features=58, bias=True)
)
2024-05-16 23:37:50,592 - Number of parameters: 10.783546 M
2024-05-16 23:37:50,592 - Initializing AdamW optimizer.
2024-05-16 23:37:50,594 - Optimizer initialized.
2024-05-16 23:37:50,594 - Starting model from scratch.
2024-05-16 23:37:50,596 - Pretraining: False
2024-05-16 23:37:50,596 - Testing pretrained model: False
2024-05-16 23:37:50,596 - Training: True
2024-05-16 23:37:50,597 - Training optimizer: AdamW
2024-05-16 23:37:50,597 - Training learning rate: 0.0003
2024-05-16 23:37:50,597 - Training epochs: 5000
2024-05-16 23:37:50,597 - Training batch size: 64
2024-05-16 23:37:50,597 - Training weight decay: 0.01
2024-05-16 23:37:50,603 - Starting training...
2024-05-16 23:38:09,213 - Epoch 1/5000
2024-05-16 23:38:09,213 -   Epoch Train Time: 18.605
2024-05-16 23:38:09,213 -   Epoch Train Loss: 4.04883099
2024-05-16 23:40:03,374 -   Evaluation train Loss: 3.46331368
2024-05-16 23:41:57,657 -   Evaluation val Loss: 3.46538004
2024-05-16 23:41:57,657 -   Evaluation Time: 228.440
2024-05-16 23:41:57,658 -   Found best model, saving model.
2024-05-16 23:42:13,247 - Epoch 2/5000
2024-05-16 23:42:13,247 -   Epoch Train Time: 13.444
2024-05-16 23:42:13,248 -   Epoch Train Loss: 3.43616724
