#### Weight & Biases settings

wandb:
  name: "DanteGPT - More epochs"
  resume: False
  job_type: "Train-Test"
  group: "Exercise 1"
  project: "University - DLA - Lab 2"
  tags: 
  - "NanoGPT"
  - "LLM"
  - "NLP"
  - "Base"
  notes: "First attempt at training a basic NanoGPT on Dante's Divina Commedia."


#### Experiment settings

config:
  # Machine
  seed: 1
  device: "cuda:0"
  cuda_benchmark: True

  # Paths and dirs
  data: "divina_commedia.txt"
  results_dir: "results"
  log_file: "log.txt"
  # hf_cache_dir: "../hf"

  model_type: "NanoGPT"
  model:
    block_size: 256  # 1024 for GPT2  # What is the maximum context length for predictions?  
    n_embed: 384  # 768 for GPT2
    n_head: 6  # 12 for GPT2
    n_layers: 6  # 12 for GPT2
    dropout: 0.2  # 0.0 for GPT2
    parallel_heads: True 
  
  criterion: "CrossEntropyLoss"
  
  dataloader:
    batch_size: 64  # how many independent sequences will we process in parallel?

  optimizer_name: "AdamW"  # Lion, Adam, SGD, Adamax, Adadelta, RMSProp ecc. (https://pytorch.org/docs/stable/optim.html)
  optimizer:
    lr: 0.0003
    weight_decay: 0.01
    amsgrad: True
  
  scheduler_name: "OneCycleLR"  # MultiStepLR
  scheduler: 
    max_lr: 0.0001
  
  pretrain: False
  pretest: False
  train: True
  generate: True

  training:
    n_epochs: 5000  # 5000
    eval_interval: 500  # 500
    eval_iters: 200  # 200
    checkpoint_every: 250
    clip_grads: False
    patience: 50
  
  generation:
    max_new_tokens: 1000


  # Goals
  problem: null
  adversarial: False
  xai: False
  explain_gradients: False
  explain_cam: False