2024-05-13 17:49:03,083 - Starting run.
2024-05-13 17:49:03,083 - Logger setup correctly.
2024-05-13 17:49:03,088 - Seed set to 1.
2024-05-13 17:49:03,088 - Log filepath: results\2024-05-12 - Test\log.txt.
2024-05-13 17:49:03,089 - Data dir: data.
2024-05-13 17:49:03,089 - Dataset: MNIST
2024-05-13 17:49:03,089 - Number of dataloader workers: 4
2024-05-13 17:49:03,089 - Network: MLP
2024-05-13 17:49:03,089 - Computation device: cuda:0
2024-05-13 17:49:03,089 - Loading dataset from "data".
2024-05-13 17:49:03,374 - Dataset loaded.
2024-05-13 17:49:03,374 - Initializing MLP model.
2024-05-13 17:49:03,617 - Model initialized.
2024-05-13 17:49:03,617 - Showing model structure:
2024-05-13 17:49:03,617 - MultiLayerPerceptron(
  (layers): Sequential(
    (linear_1): Linear(in_features=784, out_features=400, bias=False)
    (bn_1): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (act_1): ReLU()
    (dropout_1): Dropout(p=0.2, inplace=False)
    (linear_2): Linear(in_features=400, out_features=150, bias=False)
    (bn_2): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (act_2): ReLU()
    (dropout_2): Dropout(p=0.2, inplace=False)
    (last_linear): Linear(in_features=150, out_features=10, bias=False)
  )
)
2024-05-13 17:49:03,617 - Initializing AdamW optimizer.
2024-05-13 17:49:03,618 - Optimizer initialized.
2024-05-13 17:49:03,627 - Starting model from scratch.
2024-05-13 17:49:03,629 - Pretraining: False
2024-05-13 17:49:03,629 - Testing pretrained model: False
2024-05-13 17:49:03,630 - Training: True
2024-05-13 17:49:03,630 - Training optimizer: AdamW
2024-05-13 17:49:03,630 - Training learning rate: 0.001
2024-05-13 17:49:03,630 - Training epochs: 1
2024-05-13 17:49:03,630 - Training batch size: 512
2024-05-13 17:49:03,630 - Training weight decay: 0.01
2024-05-13 17:49:03,632 - Starting training...
