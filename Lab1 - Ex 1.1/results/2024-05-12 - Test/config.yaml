#### Weight & Biases settings

wandb:
  name: "2024-05-12 - Test"
  resume: False
  job_type: "Train-Test"
  group: "Exercise 1.1"
  project: "University - DLA - Lab 1"
  tags: 
  - "MLP"  # Model
  - "Base"  # Type
  notes: "First attempt at training a basic MLP on MNIST dataset."


#### Experiment settings

config:
  # Machine
  seed: 1
  device: "cuda:0"

  # Paths and dirs
  data_dir: "data"
  results_dir: "results"
  log_file: "log.txt"

  model_type: "MLP"
  model:
    input_size: 784  # 28*28 (MNIST size)
    n_hidden_layers: 2
    hidden_layer_sizes: [400, 150]
    output_size: 10  # Number of output classes
    activation: "ReLU"
    batch_norm: True
    dropout: 0.2  # If not using dropout, set this to null (null is the YAML equivalent for None)

  criterion: "CrossEntropyLoss"

  dataset:
    dataset_name: "MNIST"
    val_size: 0.2
    val_shuffle: True
    normalize: True
    augment: True
    use_sampler: True

  dataloader:
    num_workers: 4
    batch_size: 512
    pin_memory: True
    persistent_workers: True

  optimizer_name: "AdamW"  # Lion, Adam, SGD, Adamax, Adadelta, RMSProp ecc. (https://pytorch.org/docs/stable/optim.html)
  optimizer:
    lr: 0.001
    weight_decay: 0.01
    amsgrad: True

  scheduler_name: "OneCycleLR"  # MultiStepLR
  scheduler: 
    max_lr: 0.01
      
  pretrain: False
  pretest: False
  train: True
  test: False

  training:
    n_epochs: 1
    eval_interval: 1
    checkpoint_every: 1
    patience: 20

  clip_grads: False

  # Goals
  problem: None
  xai: False
  explain_gradients: False
  explain_cam: False