#### Weight & Biases settings

name: "2024-05-13 - Bugfix"
job_type: "Train-Test"
group: "Exercise 1.1"
project: "University - DLA - Lab 1"
tags: 
- "MLP"  # Model
- "Base"  # Type
notes: "First attempt at training a basic MLP on MNIST dataset."

resume: False

#### Experiment settings

# Paths and dirs
data_dir: "data"
results_dir: "results"
log_file: "log.txt"

# Model, data, optimization configuration
config:
  architecture: "MLP"
  input_size: 784  # 28*28 (MNIST size)
  n_hidden_layers: 2
  hidden_layer_sizes: [400, 150]
  n_classes: 10
  activation: "ReLU"
  batch_norm: True
  dropout: True
  dropout_prob: 0.2

  dataset_name: "MNIST"
  val_size: 0.2
  val_shuffle: True
  normalize: True
  augment: True
  use_sampler: True
  batch_size: 512

  criterion: "CrossEntropyLoss"

  optimizer_name: "AdamW"  # Lion, Adam, SGD, Adamax, Adadelta, ecc. (https://pytorch.org/docs/stable/optim.html)
  optimizer_kw:
    lr: 0.001
    weight_decay: 0.01
    amsgrad: True
    
  scheduler: "OneCycleLR"
  scheduler_kw:
    max_lr: 0.01
    
  clip_grads: False

  n_epochs: 150
  eval_interval: 1
  checkpoint_every: 1
  patience: 20

  pretrain: False
  pretest: False
  train: False
  test: True

# Computing settings
seed: 1
device: "cuda:0"
n_workers_dataloader: 0

# Goals
problem: None
xai: False
explain_gradients: False
explain_cam: False