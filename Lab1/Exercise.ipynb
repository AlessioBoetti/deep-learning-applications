{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import ConvolutionalNeuralNetworks\n",
    "from utils import (get_loaders, save_checkpoint, load_checkpoint, \n",
    "                   load_best_model, metrics, eval_fn, \n",
    "                   create_directory_if_does_not_exist, EarlyStopping, Lion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super(FirstConvLayer, self).__init__()\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n",
    "\n",
    "\n",
    "class ConvolutionalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, want_shortcut, downsample, last_layer, pool_type):\n",
    "        super(ConvolutionalBlock, self).__init__()\n",
    "\n",
    "        self.want_shortcut = want_shortcut\n",
    "        if self.want_shortcut:\n",
    "            self.shortcut == nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels,\n",
    "                          kernel_size=1, stride=2, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLu(),\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                      kernel_size=3, padding='same', bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLu()\n",
    "        )\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                               kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        if downsample:\n",
    "            if last_layer:\n",
    "                self.want_shortcut = False\n",
    "                self.sequential.append(nn.AdaptiveMaxPool2d(2))\n",
    "            else:\n",
    "                if pool_type == 'convolution':\n",
    "                    self.conv1 = nn.Conv2d(\n",
    "                        in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "                elif pool_type == 'kmax':\n",
    "                    channels = [64, 128, 256, 512]\n",
    "                    dimension = [511, 256, 128]\n",
    "                    index = channels.index(in_channels)\n",
    "                    self.sequential.append(\n",
    "                        nn.AdaptiveMaxPool2d(dimension[index]))\n",
    "                else:\n",
    "                    self.sequential.append(nn.MaxPool2d(\n",
    "                        kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "        self.relu = nn.ReLu()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.want_shortcut:\n",
    "            short = x\n",
    "            out = self.conv1(x)\n",
    "            out = self.sequential(out)\n",
    "            if out.shape != short.shape:\n",
    "                short = self.shortcut(short)\n",
    "            out = self.relu(short + out)\n",
    "            return out\n",
    "        else:\n",
    "            out = self.conv1(x)\n",
    "            return self.sequential(out)\n",
    "\n",
    "\n",
    "class FullyConnectedBlock(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super(FullyConnectedBlock, self).__init__()\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLu(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLu(),\n",
    "            nn.Linear(1024, n_class),\n",
    "            # nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lion(optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-4, betas=(0.9, 0.99), weight_decay=0.0):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError('Invalid learning rate: {}'.format(lr))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError('Invalid beta parameter at index 0: {}'.format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError('Invalid beta parameter at index 1: {}'.format(betas[1]))\n",
    "        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "        \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                p.data.mul_(1 - group['lr'] * group['weight_decay'])\n",
    "\n",
    "                grad = p.grad\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state['exp_avg'] = torch.zeros_like(p)\n",
    "\n",
    "                exp_avg = state['exp_avg']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                update = exp_avg * beta1 + grad * (1 - beta1)\n",
    "                p.add_(torch.sign(update), alpha=-group['lr'])\n",
    "                exp_avg.mul_(beta2).add_(grad, alpha=1 - beta2)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(epoch, loader, model, optimizer, scheduler, loss_fn, scaler, metric_collection, device):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "\n",
    "    for (data, target) in tqdm(loader, desc=f\"Epoch {epoch + 1}\"):\n",
    "        data = data.to(device, non_blocking=True)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "\n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast():\n",
    "            prediction = model(data)\n",
    "            loss = loss_fn(prediction, target)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.updata()\n",
    "        scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "        metric_collection(prediction, target)\n",
    "\n",
    "    train_loss = running_loss / len(loader)\n",
    "    train_accuracy = metric_collection(\"MulticlassAccuracy\").compute().cpu() * 100\n",
    "    \n",
    "    metric_collection.reset()\n",
    "\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "\n",
    "def main(wb, checkpoint_dir, weight_dir, device, num_workers):\n",
    "    model = ConvolutionalNeuralNetworks(depth=wb.config['depth'],\n",
    "                                        n_classes=wb.config['n_class'],\n",
    "                                        want_shortcut=wb.config['want_shortcut'],\n",
    "                                        pool_type=wb.config['pool_type']).to(device)\n",
    "    \n",
    "    optimizer = Lion(model.parameters(), lr=wb.config['learning_rate'], weight_decay=wb.config['weight_decay'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    metric_collection = metrics(wb, device)\n",
    "\n",
    "    if wb.config['evaluation']:\n",
    "        load_best_model(torch.load(weight_dir), model)\n",
    "        test_loader = get_loaders(wb.config['batch_size'], num_workers, training=False)\n",
    "        eval_fn(test_loader, model, criterion, metric_collection, device)\n",
    "        sys.exit()\n",
    "\n",
    "    if wb.resumed:\n",
    "        start, monitored_value, count = load_checkpoint(torch.load(checkpoint_dir), model, optimizer)\n",
    "        patience = EarlyStopping('max', wb.config['patience'], count, monitored_value)\n",
    "    else:\n",
    "        start = 0\n",
    "        patience = EarlyStopping('max', wb.config['patience'])\n",
    "\n",
    "    train_loader, test_loader = get_loaders(wb.config['batch_size'], num_workers)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                                    max_lr=wb.config['max_lr'],\n",
    "                                                    steps_per_epoch=len(train_loader),\n",
    "                                                    epochs=wb.config['num_epochs'] - start)\n",
    "\n",
    "    wb.watch(model, log=\"all\")\n",
    "\n",
    "    for epoch in range(start, wb.config['num_epochs']):\n",
    "        train_loss, train_accuracy = train_fn(epoch, train_loader, model, optimizer, scheduler, criterion, scaler,\n",
    "                                              metric_collection, device)\n",
    "        \n",
    "        test_loss, test_accuracy = eval_fn(test_loader, model, criterion, metric_collection, device)\n",
    "\n",
    "        wb.log({'train_loss': train_loss,\n",
    "                'train_accuracy': train_accuracy,\n",
    "                'test_loss': test_loss,\n",
    "                'test_accuracy': test_accuracy\n",
    "                })\n",
    "        \n",
    "        # save best model\n",
    "        if patience(test_accuracy):\n",
    "            wb.log({\n",
    "                \"accuracy_epoch\": epoch,\n",
    "            })\n",
    "            checkpoint = {\"state_dict\": model.state_dict()}\n",
    "            save_checkpoint(\"=> Best model found\", checkpoint, weight_dir)\n",
    "\n",
    "        # save checkpoint\n",
    "        checkpoint = {\n",
    "            'start': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'max_accuracy': getattr(patience, 'baseline'),\n",
    "            'count': getattr(patience, 'count'),\n",
    "        }\n",
    "        save_checkpoint('=> Saving checkpoint', checkpoint, checkpoint_dir)\n",
    "\n",
    "        # early stopping\n",
    "        if getattr(patience, 'count') == 0:\n",
    "            print('=> Patience finished')\n",
    "            break\n",
    "\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "def save_checkpoint(string, state, directory):\n",
    "    print(string)\n",
    "    torch.save(state, \"\".join([directory, \"model.pth.tar\"]))\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return checkpoint['start'], checkpoint['max_accuracy'], checkpoint['count']\n",
    "\n",
    "\n",
    "def create_directory_if_does_not_exist(dirs):\n",
    "    for dir in dirs:\n",
    "        if not os.path.exists(dir):\n",
    "            os.makedirs(dir)\n",
    "\n",
    "\n",
    "def load_best_model(checkpoint, model):\n",
    "    print(\"Loading best model\")\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "\n",
    "def get_loaders(batch_size, num_workers, training=True):\n",
    "    transform = T.Compose([\n",
    "        T.AutoAugment(policy=T.AutoAugmentPolicy.CIFAR10),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
    "    \n",
    "    test_ds = CIFAR10(\n",
    "        root='./Dataset/',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "\n",
    "    if training:\n",
    "        train_ds = CIFAR10(\n",
    "            root='./Dataset/',\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transform\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "            shuffle=False,\n",
    "            persistent_workers=True,\n",
    "        )\n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    return test_loader\n",
    "\n",
    "\n",
    "def metrics(wb, device):\n",
    "    metric_collection = torchmetrics.MetricCollection([\n",
    "        torchmetrics.classification.MulticlassAccuracy(num_classes=wb.config['num_class']).to(device=device)\n",
    "    ])\n",
    "    wb.define_metric(\"test_loss\", summary=\"min\")\n",
    "    wb.define_metric(\"test_accuracy\", summary=\"max\")\n",
    "    wb.define_metric(\"accuracy_epoch\")\n",
    "    return metric_collection\n",
    "\n",
    "\n",
    "def eval_fn(loader, model, criterion, metric_collection, device):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data = data.to(device, non_blocking=True)\n",
    "            target = target.to(device, non_blocking=True)\n",
    "            prediction = model(data)\n",
    "            loss = criterion(prediction, target)\n",
    "            running_loss += loss.item()\n",
    "            metric_collection(prediction, target)\n",
    "\n",
    "    loss = running_loss / len(loader)\n",
    "    accuracy = metric_collection['MulticlassAccurcay'].compute().cpu() * 100\n",
    "\n",
    "    print(f\"Got on test set --> Accuracy: {accuracy:.3f} and Loss: {loss:.3f}\")\n",
    "\n",
    "    metric_collection.reset()\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "def save_plot(train_l, train_a, test_l, test_a):\n",
    "    plt.plot(train_a, \"-\")\n",
    "    plt.plot(test_a, \"-\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.legend([\"Train\", \"Valid\"])\n",
    "    plt.title(\"Train Vs. Valid Accuracy\")\n",
    "    plt.savefig('result/accuracy')\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(train_l, \"-\")\n",
    "    plt.plot(test_l, \"-\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"losses\")\n",
    "    plt.legend([\"Train\", \"Valid\"])\n",
    "    plt.title(\"Train Vs. Valid Losses\")\n",
    "    plt.savefig('result/losses')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, mod, patience, count=None, baseline=None):\n",
    "        self.patience = patience\n",
    "        self.count = patience if count is None else count\n",
    "        if mod == 'max':\n",
    "            self.baseline = 0\n",
    "            self.operation = self.max\n",
    "        if mod == 'min':\n",
    "            self.baseline = baseline\n",
    "            self.operation = self.min\n",
    "\n",
    "\n",
    "    def max(self, monitored_value):\n",
    "        if monitored_value > self.baseline:\n",
    "            self.baseline = monitored_value\n",
    "            self.count = self.patience\n",
    "            return True\n",
    "        else:\n",
    "            self.count -= 1\n",
    "            return False\n",
    "        \n",
    "    \n",
    "    def min(self, monitored_value):\n",
    "        if monitored_value < self.baseline:\n",
    "            self.baseline = monitored_value\n",
    "            self.count = self.patience\n",
    "            return True\n",
    "        else:\n",
    "            self.count -= 1\n",
    "            return False\n",
    "        \n",
    "    \n",
    "    def __call__(self, monitored_value):\n",
    "        return self.operation(monitored_value)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "wab = wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"DLA - Lab1\",\n",
    "    # group=\"Experiment\",\n",
    "    tags=[],\n",
    "    resume=False,\n",
    "    name=\"depth-48-skip\",\n",
    "    config={\n",
    "        # model parameters\n",
    "        \"architecture\": \"Convolutional Neural Networks\",\n",
    "        \"depth\": 48,\n",
    "        \"n_class\": 10,\n",
    "        \"want_shortcut\": True,\n",
    "        \"pool_type\": \"max\",\n",
    "\n",
    "        # datasets\n",
    "        \"dataset\": \"CIFAR-10\",\n",
    "\n",
    "        # hyperparameters\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"batch_size\": 2048,\n",
    "        \"optimizer\": \"Lion\",\n",
    "        \"weight_decay\": 1e-2,\n",
    "        \"scheduler\": \"One Cycle Learning\",\n",
    "        \"max_lr\": 5e-4,\n",
    "        \"num_epochs\": 200,\n",
    "        \"patience\": 20,\n",
    "\n",
    "        # run type\n",
    "        \"evaluation\": False,\n",
    "    })\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    def __init__(self, dict):\n",
    "        self.config = dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "        # model parameters\n",
    "        \"architecture\": \"CNN\",\n",
    "        \"depth\": 48,\n",
    "        \"n_class\": 10,\n",
    "        \"want_shortcut\": True,\n",
    "        \"pool_type\": \"max\",\n",
    "\n",
    "        # dataset\n",
    "        \"dataset\": \"CIFAR-10\",\n",
    "\n",
    "        # hyperparameters\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"batch_size\": 2048,\n",
    "        \"optimizer\": \"Lion\",\n",
    "        \"weight_decay\": 1e-2,\n",
    "        \"scheduler\": \"One Cycle Learning\",\n",
    "        \"max_lr\": 5e-4,\n",
    "        \"num_epochs\": 200,\n",
    "        \"patience\": 20,\n",
    "\n",
    "        # run type\n",
    "        \"evaluation\": False,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wab = Parameters(parameters)\n",
    "checkpoint_dir = \"\".join([\"checkpoint/\"])\n",
    "results_dir = \"\".join([\"results/\", wab.config['architecture'], '/'])\n",
    "create_directory_if_does_not_exist([checkpoint_dir, results_dir])\n",
    "dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "n_workers = 16\n",
    "main(wab, checkpoint_dir, results_dir, dev, n_workers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".DLA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
